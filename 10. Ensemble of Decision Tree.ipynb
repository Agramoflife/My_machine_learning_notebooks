{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04022685",
   "metadata": {},
   "source": [
    "# Ensemble of Decision "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6d15f6",
   "metadata": {},
   "source": [
    "## how does ensemble models reduces the overfitting issue \n",
    "in decsion tree the split happens based on the featuer which gives us the maximum information gain or which gives us max homogeneous now because of this it will always give importance to those features and it will fail to generalize hence becomes over fit when we use the ensemble technique where we sample the data with replacement this gives every feature in the decision tree to have a split and the depth of the tree is also kept low hence it does not overfit and kind of generalizes wells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fec34ce",
   "metadata": {},
   "source": [
    "ensemble means combination i.e combination of decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05be6550",
   "metadata": {},
   "source": [
    "bagging is bootstraping and aggregating "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604314ed",
   "metadata": {},
   "source": [
    "replaceable sampling - consider you have a list of playcards with name write on it so you randomly pick a playcard see the name write it down and place the playcard back to where you picked form this is similar to replaceable sampling you have a data you pick a random datapoint from the dataset and put in it data1 and then put back the picked datapoint from where it belonged, the number can repeat in such case as you are putting the datapoint back to where it belong , this is also called a row sampling you are picking rows and putting it back to where it belong , so you can have duplicate entries in you sampled dataset similarly we can have column sampling but without replacement "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b309ab",
   "metadata": {},
   "source": [
    "In a Random Forest algorithm, there are indeed two levels of sampling involved: replaceable row (sample with replacement) and non-replaceable column (feature) sampling. These sampling techniques help make Random Forests robust and reduce overfitting.\n",
    "\n",
    "1. **Replaceable Row Sampling (Bootstrap Aggregating - Bagging):**\n",
    "   \n",
    "   In the Random Forest algorithm, each individual decision tree is constructed from a different subset of the training data. This is achieved through a process called bootstrap aggregating or bagging. Bagging involves randomly sampling the training data with replacement. This means that some data points from the original dataset will appear multiple times in the subsets used to train each tree, while others may not appear at all. This sampling with replacement helps reduce the variance of the model and makes it less prone to overfitting.\n",
    "\n",
    "2. **Non-Replaceable Column Sampling (Feature Sampling):**\n",
    "\n",
    "   In addition to sampling rows (data points), Random Forests also introduce an additional level of randomness by randomly selecting a subset of features (columns) at each split point in each decision tree. This process is often referred to as feature sampling or feature bagging. By doing this, Random Forests ensure that no single feature dominates the decision-making process of any tree. It encourages diversity among the trees, which in turn helps improve the overall generalization performance of the ensemble.\n",
    "\n",
    "   Typically, a square root of the total number of features or a user-defined number of features is randomly chosen for each split. For example, if you have 16 features, you might randomly select 4 features for consideration at each split.\n",
    "\n",
    "These two levels of sampling, replaceable row sampling (bagging) and non-replaceable column sampling (feature sampling), work together to create a diverse set of decision trees in the forest, each trained on a slightly different subset of data and features. This diversity is a key factor in the Random Forest's ability to reduce overfitting and provide robust and accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4792d212",
   "metadata": {},
   "source": [
    "**Important: It can be theoretically shown that the variance of the combined predictions are reduced to 1/n (n: number of classifiers) of the original variance, under some assumptions. (Think Central Limit Theorem)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0437c06",
   "metadata": {},
   "source": [
    "So decision tree are basically low bias and high variance why because for different root nodes the decision is different i.e the model has a very high variance and very low bais it fails to generalize "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7f8a73",
   "metadata": {},
   "source": [
    "so to overcome the disadvantage of decision tree we take multiple decision tree via resampling and each decision treen in the random forest decrease the variance by 1/n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dc989e",
   "metadata": {},
   "source": [
    "in boosting conisder this you are delving more and more into features like the example of cat it it has fur then cat the it classifed dog also as cat now you delve deeper if fur and pink nose then cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e29f946",
   "metadata": {},
   "source": [
    "In boosting we model the decision tree with feaures and residuals i.e we pass features and error well we do start with features and target variable at the start thouhg "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983547e3",
   "metadata": {},
   "source": [
    "stacking is using different models for bagging "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375e2845",
   "metadata": {},
   "source": [
    "Stacking, also known as stacked generalization, is an ensemble machine learning technique that combines multiple models (learners) to improve predictive performance. It is a meta-learning method that leverages the strengths of different algorithms by using their predictions as input features for a final, higher-level model called a meta-learner or a blender. The goal of stacking is to create a more robust and accurate predictive model by harnessing the diversity of various base models.\n",
    "\n",
    "Here's a general overview of how stacking works:\n",
    "\n",
    "1. **Base Models**: Several diverse machine learning models are trained independently on the same dataset. These base models can be different algorithms (e.g., decision trees, support vector machines, random forests, neural networks) or the same algorithm with different hyperparameters.\n",
    "\n",
    "2. **Predictions**: Once the base models are trained, they each make predictions on the same dataset, typically the validation set or a holdout set.\n",
    "\n",
    "3. **Meta-learner**: The predictions generated by the base models become input features for a meta-learner. This meta-learner is another machine learning model, often simple and interpretable, like a linear regression, logistic regression, or a decision tree.\n",
    "\n",
    "4. **Training the Meta-learner**: The meta-learner takes the predictions from the base models as input features and is trained to make predictions based on these inputs while using the actual target values (ground truth) from the training dataset.\n",
    "\n",
    "5. **Making Predictions**: Once the meta-learner is trained, it can be used to make predictions on new, unseen data.\n",
    "\n",
    "The idea behind stacking is that it can capture complex relationships between the base models' predictions and the target variable that may not be apparent to any individual base model. By combining the predictions of multiple models and letting the meta-learner learn how to weigh and use these predictions effectively, stacking often achieves better predictive performance compared to using individual models alone.\n",
    "\n",
    "However, it's important to note that stacking can be computationally expensive and may require a larger amount of data to avoid overfitting. Careful cross-validation and hyperparameter tuning are essential to ensure the effectiveness of the stacking ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c549795",
   "metadata": {},
   "source": [
    "Stacking is typically associated with the bagging (Bootstrap Aggregating) ensemble technique, not with the boosting ensemble technique. Here's why:\n",
    "\n",
    "1. **Bagging (Bootstrap Aggregating)**: Bagging involves training multiple base models independently on random subsets of the training data (bootstrap samples) and then combining their predictions, often by averaging or voting, to reduce overfitting and improve generalization. Stacking can be applied to bagging by using the predictions of these independently trained base models as input features for a meta-learner, which learns to make predictions based on these inputs. This combination of bagging and stacking is sometimes referred to as \"stacked bagging.\"\n",
    "\n",
    "2. **Boosting**: Boosting, on the other hand, is an ensemble technique where base models are trained sequentially, and each subsequent model is trained to correct the errors made by the previous ones. The final prediction is typically a weighted sum of the predictions from these base models. Boosting algorithms like AdaBoost, Gradient Boosting, and XGBoost do not directly incorporate stacking because the focus in boosting is on building a strong sequential model by adjusting the weights of the data points and base models.\n",
    "\n",
    "In summary, stacking is primarily associated with bagging ensembles, where independently trained base models' predictions are combined using a meta-learner. Boosting, on the other hand, follows a different strategy and doesn't typically involve stacking in the same way. However, both bagging and boosting can benefit from ensembling techniques like stacking or blending when you want to combine different ensemble methods or multiple layers of models for further improvement in predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c81207d",
   "metadata": {},
   "source": [
    "# Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d79d6b2",
   "metadata": {},
   "source": [
    "- Suppose you are using a bagging based algorithm say a RandomForest in model building. Which of the following can be true? 1)  Number of tree should be as large as possible 2) You will have interpretability after using RandomForest Ans = 1\n",
    "\n",
    "- Which of the following is true about “max_depth” hyperparameter in Gradient Boosting? 1) Lower is better parameter in case of same validation accuracy 2) Higher is better parameter in case of same validation accuracy 3) Increase the value of max_depth may overfit the data 4) Increase the value of max_depth may underfit the data? Ans = 1 and 3\n",
    "\n",
    "- In boosting trees, individual weak learners are independent of each other ? Ans False\n",
    "\n",
    "- Which of the following is/are true about bagging trees? 1) In bagging trees, individual trees are independent of each other 2) Bagging is the method for improving the performance by aggregating the results of weak learners ? Ans all \n",
    "\n",
    "- Which of the following is true about the Gradient Boosting trees? In each stage, introduce a new regression tree to compensate the shortcomings of existing model We can use gradient decent method for minimize the loss function? Ans all\n",
    "\n",
    "- In case of limited training data, which technique, bagging or stacking, would be preferred, and why?\n",
    "Your Answer\n",
    "Bagging, because we can combine as many classifier as we want by training each on a different sample of the training data\n",
    "Correct Answer\n",
    "Stacking, because each classifier is trained on all of the available data\n",
    "\n",
    "- How do you improve random forest accuracy? ans \n",
    "Algorithm Tuning\n",
    "Add more data\n",
    "Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "178e6867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from numpy import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed6c0674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Deepu\\\\Documents\\\\machine_learning'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e990848f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Deepu\\Downloads\n"
     ]
    }
   ],
   "source": [
    "cd C:\\\\Users\\\\Deepu\\\\Downloads "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd89b081",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c904f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>V29</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.114697</td>\n",
       "      <td>0.796303</td>\n",
       "      <td>-0.149553</td>\n",
       "      <td>-0.823011</td>\n",
       "      <td>0.878763</td>\n",
       "      <td>-0.553152</td>\n",
       "      <td>0.939259</td>\n",
       "      <td>-0.108502</td>\n",
       "      <td>0.111137</td>\n",
       "      <td>-0.390521</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.335776</td>\n",
       "      <td>-0.807853</td>\n",
       "      <td>-0.055940</td>\n",
       "      <td>-1.025281</td>\n",
       "      <td>-0.369557</td>\n",
       "      <td>0.204653</td>\n",
       "      <td>0.242724</td>\n",
       "      <td>0.085713</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.039318</td>\n",
       "      <td>0.495784</td>\n",
       "      <td>-0.810884</td>\n",
       "      <td>0.546693</td>\n",
       "      <td>1.986257</td>\n",
       "      <td>4.386342</td>\n",
       "      <td>-1.344891</td>\n",
       "      <td>-1.743736</td>\n",
       "      <td>-0.563103</td>\n",
       "      <td>-0.616315</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.377003</td>\n",
       "      <td>-0.072200</td>\n",
       "      <td>-0.197573</td>\n",
       "      <td>1.014807</td>\n",
       "      <td>1.011293</td>\n",
       "      <td>-0.167684</td>\n",
       "      <td>0.113136</td>\n",
       "      <td>0.256836</td>\n",
       "      <td>85.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.275706</td>\n",
       "      <td>-1.531508</td>\n",
       "      <td>-1.021969</td>\n",
       "      <td>-1.602152</td>\n",
       "      <td>-1.220329</td>\n",
       "      <td>-0.462376</td>\n",
       "      <td>-1.196485</td>\n",
       "      <td>-0.147058</td>\n",
       "      <td>-0.950224</td>\n",
       "      <td>1.560463</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.193271</td>\n",
       "      <td>-0.103533</td>\n",
       "      <td>0.150945</td>\n",
       "      <td>-0.811083</td>\n",
       "      <td>-0.197913</td>\n",
       "      <td>-0.128446</td>\n",
       "      <td>0.014197</td>\n",
       "      <td>-0.051289</td>\n",
       "      <td>42.70</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.940137</td>\n",
       "      <td>-0.357671</td>\n",
       "      <td>-1.210551</td>\n",
       "      <td>0.382523</td>\n",
       "      <td>0.050823</td>\n",
       "      <td>-0.171322</td>\n",
       "      <td>-0.109124</td>\n",
       "      <td>-0.002115</td>\n",
       "      <td>0.869258</td>\n",
       "      <td>-0.001965</td>\n",
       "      <td>...</td>\n",
       "      <td>0.157994</td>\n",
       "      <td>0.650355</td>\n",
       "      <td>0.034206</td>\n",
       "      <td>0.739535</td>\n",
       "      <td>0.223605</td>\n",
       "      <td>-0.195509</td>\n",
       "      <td>-0.012791</td>\n",
       "      <td>-0.056841</td>\n",
       "      <td>29.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.081395</td>\n",
       "      <td>-0.502615</td>\n",
       "      <td>1.075887</td>\n",
       "      <td>-0.543359</td>\n",
       "      <td>-1.472946</td>\n",
       "      <td>-1.065484</td>\n",
       "      <td>-0.443231</td>\n",
       "      <td>-0.143374</td>\n",
       "      <td>1.659826</td>\n",
       "      <td>-1.131238</td>\n",
       "      <td>...</td>\n",
       "      <td>0.224157</td>\n",
       "      <td>0.821209</td>\n",
       "      <td>-0.137223</td>\n",
       "      <td>0.986259</td>\n",
       "      <td>0.563228</td>\n",
       "      <td>-0.574206</td>\n",
       "      <td>0.089673</td>\n",
       "      <td>0.052036</td>\n",
       "      <td>68.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0  0.114697  0.796303 -0.149553 -0.823011  0.878763 -0.553152  0.939259   \n",
       "1 -0.039318  0.495784 -0.810884  0.546693  1.986257  4.386342 -1.344891   \n",
       "2  2.275706 -1.531508 -1.021969 -1.602152 -1.220329 -0.462376 -1.196485   \n",
       "3  1.940137 -0.357671 -1.210551  0.382523  0.050823 -0.171322 -0.109124   \n",
       "4  1.081395 -0.502615  1.075887 -0.543359 -1.472946 -1.065484 -0.443231   \n",
       "\n",
       "         V8        V9       V10  ...       V21       V22       V23       V24  \\\n",
       "0 -0.108502  0.111137 -0.390521  ... -0.335776 -0.807853 -0.055940 -1.025281   \n",
       "1 -1.743736 -0.563103 -0.616315  ... -1.377003 -0.072200 -0.197573  1.014807   \n",
       "2 -0.147058 -0.950224  1.560463  ... -0.193271 -0.103533  0.150945 -0.811083   \n",
       "3 -0.002115  0.869258 -0.001965  ...  0.157994  0.650355  0.034206  0.739535   \n",
       "4 -0.143374  1.659826 -1.131238  ...  0.224157  0.821209 -0.137223  0.986259   \n",
       "\n",
       "        V25       V26       V27       V28    V29  Target  \n",
       "0 -0.369557  0.204653  0.242724  0.085713   0.89       0  \n",
       "1  1.011293 -0.167684  0.113136  0.256836  85.00       0  \n",
       "2 -0.197913 -0.128446  0.014197 -0.051289  42.70       0  \n",
       "3  0.223605 -0.195509 -0.012791 -0.056841  29.99       0  \n",
       "4  0.563228 -0.574206  0.089673  0.052036  68.00       0  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09bb150e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56962, 30)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c62cfd21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 56962 entries, 0 to 56961\n",
      "Data columns (total 30 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   V1      56962 non-null  float64\n",
      " 1   V2      56962 non-null  float64\n",
      " 2   V3      56962 non-null  float64\n",
      " 3   V4      56962 non-null  float64\n",
      " 4   V5      56962 non-null  float64\n",
      " 5   V6      56962 non-null  float64\n",
      " 6   V7      56962 non-null  float64\n",
      " 7   V8      56962 non-null  float64\n",
      " 8   V9      56962 non-null  float64\n",
      " 9   V10     56962 non-null  float64\n",
      " 10  V11     56962 non-null  float64\n",
      " 11  V12     56962 non-null  float64\n",
      " 12  V13     56962 non-null  float64\n",
      " 13  V14     56962 non-null  float64\n",
      " 14  V15     56962 non-null  float64\n",
      " 15  V16     56962 non-null  float64\n",
      " 16  V17     56962 non-null  float64\n",
      " 17  V18     56962 non-null  float64\n",
      " 18  V19     56962 non-null  float64\n",
      " 19  V20     56962 non-null  float64\n",
      " 20  V21     56962 non-null  float64\n",
      " 21  V22     56962 non-null  float64\n",
      " 22  V23     56962 non-null  float64\n",
      " 23  V24     56962 non-null  float64\n",
      " 24  V25     56962 non-null  float64\n",
      " 25  V26     56962 non-null  float64\n",
      " 26  V27     56962 non-null  float64\n",
      " 27  V28     56962 non-null  float64\n",
      " 28  V29     56962 non-null  float64\n",
      " 29  Target  56962 non-null  int64  \n",
      "dtypes: float64(29), int64(1)\n",
      "memory usage: 13.0 MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bc2b2be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>V29</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>56962.000000</td>\n",
       "      <td>56962.000000</td>\n",
       "      <td>56962.000000</td>\n",
       "      <td>56962.000000</td>\n",
       "      <td>56962.000000</td>\n",
       "      <td>56962.000000</td>\n",
       "      <td>56962.000000</td>\n",
       "      <td>56962.000000</td>\n",
       "      <td>56962.000000</td>\n",
       "      <td>56962.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>56962.000000</td>\n",
       "      <td>56962.000000</td>\n",
       "      <td>56962.000000</td>\n",
       "      <td>56962.000000</td>\n",
       "      <td>56962.000000</td>\n",
       "      <td>56962.000000</td>\n",
       "      <td>56962.000000</td>\n",
       "      <td>56962.000000</td>\n",
       "      <td>56962.000000</td>\n",
       "      <td>56962.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.013286</td>\n",
       "      <td>0.006610</td>\n",
       "      <td>-0.004263</td>\n",
       "      <td>0.001496</td>\n",
       "      <td>-0.003508</td>\n",
       "      <td>-0.003079</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>-0.006501</td>\n",
       "      <td>0.001564</td>\n",
       "      <td>0.003176</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002253</td>\n",
       "      <td>-0.004938</td>\n",
       "      <td>0.004008</td>\n",
       "      <td>-0.001017</td>\n",
       "      <td>-0.000872</td>\n",
       "      <td>0.004513</td>\n",
       "      <td>0.001385</td>\n",
       "      <td>-0.001990</td>\n",
       "      <td>87.658797</td>\n",
       "      <td>0.001720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.941231</td>\n",
       "      <td>1.611224</td>\n",
       "      <td>1.516853</td>\n",
       "      <td>1.419107</td>\n",
       "      <td>1.431731</td>\n",
       "      <td>1.359647</td>\n",
       "      <td>1.301800</td>\n",
       "      <td>1.201138</td>\n",
       "      <td>1.103688</td>\n",
       "      <td>1.095156</td>\n",
       "      <td>...</td>\n",
       "      <td>0.735871</td>\n",
       "      <td>0.730301</td>\n",
       "      <td>0.621623</td>\n",
       "      <td>0.604191</td>\n",
       "      <td>0.521003</td>\n",
       "      <td>0.481857</td>\n",
       "      <td>0.416750</td>\n",
       "      <td>0.325640</td>\n",
       "      <td>258.042879</td>\n",
       "      <td>0.041443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-36.510583</td>\n",
       "      <td>-47.429676</td>\n",
       "      <td>-48.325589</td>\n",
       "      <td>-5.560118</td>\n",
       "      <td>-113.743307</td>\n",
       "      <td>-20.054615</td>\n",
       "      <td>-28.215112</td>\n",
       "      <td>-41.484823</td>\n",
       "      <td>-9.481456</td>\n",
       "      <td>-20.949192</td>\n",
       "      <td>...</td>\n",
       "      <td>-21.620120</td>\n",
       "      <td>-8.593642</td>\n",
       "      <td>-32.828995</td>\n",
       "      <td>-2.822684</td>\n",
       "      <td>-8.696627</td>\n",
       "      <td>-1.778061</td>\n",
       "      <td>-8.878665</td>\n",
       "      <td>-15.430084</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.908591</td>\n",
       "      <td>-0.600208</td>\n",
       "      <td>-0.893961</td>\n",
       "      <td>-0.847617</td>\n",
       "      <td>-0.694059</td>\n",
       "      <td>-0.770145</td>\n",
       "      <td>-0.551096</td>\n",
       "      <td>-0.211190</td>\n",
       "      <td>-0.642072</td>\n",
       "      <td>-0.535121</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.230210</td>\n",
       "      <td>-0.549030</td>\n",
       "      <td>-0.160179</td>\n",
       "      <td>-0.356599</td>\n",
       "      <td>-0.315738</td>\n",
       "      <td>-0.322766</td>\n",
       "      <td>-0.070204</td>\n",
       "      <td>-0.052351</td>\n",
       "      <td>5.615000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.036251</td>\n",
       "      <td>0.061840</td>\n",
       "      <td>0.170910</td>\n",
       "      <td>-0.022094</td>\n",
       "      <td>-0.051828</td>\n",
       "      <td>-0.278939</td>\n",
       "      <td>0.039155</td>\n",
       "      <td>0.019903</td>\n",
       "      <td>-0.052607</td>\n",
       "      <td>-0.095676</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032379</td>\n",
       "      <td>-0.000329</td>\n",
       "      <td>-0.009334</td>\n",
       "      <td>0.040045</td>\n",
       "      <td>0.018074</td>\n",
       "      <td>-0.047859</td>\n",
       "      <td>0.001451</td>\n",
       "      <td>0.011354</td>\n",
       "      <td>21.900000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.319727</td>\n",
       "      <td>0.801163</td>\n",
       "      <td>1.016897</td>\n",
       "      <td>0.738688</td>\n",
       "      <td>0.615454</td>\n",
       "      <td>0.392801</td>\n",
       "      <td>0.569769</td>\n",
       "      <td>0.324905</td>\n",
       "      <td>0.599634</td>\n",
       "      <td>0.453059</td>\n",
       "      <td>...</td>\n",
       "      <td>0.184809</td>\n",
       "      <td>0.524484</td>\n",
       "      <td>0.149985</td>\n",
       "      <td>0.437657</td>\n",
       "      <td>0.348895</td>\n",
       "      <td>0.245286</td>\n",
       "      <td>0.090737</td>\n",
       "      <td>0.077483</td>\n",
       "      <td>77.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.446505</td>\n",
       "      <td>18.902453</td>\n",
       "      <td>3.985446</td>\n",
       "      <td>15.304184</td>\n",
       "      <td>29.016124</td>\n",
       "      <td>73.301626</td>\n",
       "      <td>120.589494</td>\n",
       "      <td>18.282168</td>\n",
       "      <td>10.370658</td>\n",
       "      <td>15.236028</td>\n",
       "      <td>...</td>\n",
       "      <td>22.614889</td>\n",
       "      <td>6.790452</td>\n",
       "      <td>20.803344</td>\n",
       "      <td>4.584549</td>\n",
       "      <td>7.519589</td>\n",
       "      <td>3.517346</td>\n",
       "      <td>31.612198</td>\n",
       "      <td>22.620072</td>\n",
       "      <td>25691.160000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 V1            V2            V3            V4            V5  \\\n",
       "count  56962.000000  56962.000000  56962.000000  56962.000000  56962.000000   \n",
       "mean       0.013286      0.006610     -0.004263      0.001496     -0.003508   \n",
       "std        1.941231      1.611224      1.516853      1.419107      1.431731   \n",
       "min      -36.510583    -47.429676    -48.325589     -5.560118   -113.743307   \n",
       "25%       -0.908591     -0.600208     -0.893961     -0.847617     -0.694059   \n",
       "50%        0.036251      0.061840      0.170910     -0.022094     -0.051828   \n",
       "75%        1.319727      0.801163      1.016897      0.738688      0.615454   \n",
       "max        2.446505     18.902453      3.985446     15.304184     29.016124   \n",
       "\n",
       "                 V6            V7            V8            V9           V10  \\\n",
       "count  56962.000000  56962.000000  56962.000000  56962.000000  56962.000000   \n",
       "mean      -0.003079      0.000141     -0.006501      0.001564      0.003176   \n",
       "std        1.359647      1.301800      1.201138      1.103688      1.095156   \n",
       "min      -20.054615    -28.215112    -41.484823     -9.481456    -20.949192   \n",
       "25%       -0.770145     -0.551096     -0.211190     -0.642072     -0.535121   \n",
       "50%       -0.278939      0.039155      0.019903     -0.052607     -0.095676   \n",
       "75%        0.392801      0.569769      0.324905      0.599634      0.453059   \n",
       "max       73.301626    120.589494     18.282168     10.370658     15.236028   \n",
       "\n",
       "       ...           V21           V22           V23           V24  \\\n",
       "count  ...  56962.000000  56962.000000  56962.000000  56962.000000   \n",
       "mean   ...     -0.002253     -0.004938      0.004008     -0.001017   \n",
       "std    ...      0.735871      0.730301      0.621623      0.604191   \n",
       "min    ...    -21.620120     -8.593642    -32.828995     -2.822684   \n",
       "25%    ...     -0.230210     -0.549030     -0.160179     -0.356599   \n",
       "50%    ...     -0.032379     -0.000329     -0.009334      0.040045   \n",
       "75%    ...      0.184809      0.524484      0.149985      0.437657   \n",
       "max    ...     22.614889      6.790452     20.803344      4.584549   \n",
       "\n",
       "                V25           V26           V27           V28           V29  \\\n",
       "count  56962.000000  56962.000000  56962.000000  56962.000000  56962.000000   \n",
       "mean      -0.000872      0.004513      0.001385     -0.001990     87.658797   \n",
       "std        0.521003      0.481857      0.416750      0.325640    258.042879   \n",
       "min       -8.696627     -1.778061     -8.878665    -15.430084      0.000000   \n",
       "25%       -0.315738     -0.322766     -0.070204     -0.052351      5.615000   \n",
       "50%        0.018074     -0.047859      0.001451      0.011354     21.900000   \n",
       "75%        0.348895      0.245286      0.090737      0.077483     77.500000   \n",
       "max        7.519589      3.517346     31.612198     22.620072  25691.160000   \n",
       "\n",
       "             Target  \n",
       "count  56962.000000  \n",
       "mean       0.001720  \n",
       "std        0.041443  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.000000  \n",
       "75%        0.000000  \n",
       "max        1.000000  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "370a3817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11',\n",
       "       'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21',\n",
       "       'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'Target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da299351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "V1        0\n",
       "V2        0\n",
       "V3        0\n",
       "V4        0\n",
       "V5        0\n",
       "V6        0\n",
       "V7        0\n",
       "V8        0\n",
       "V9        0\n",
       "V10       0\n",
       "V11       0\n",
       "V12       0\n",
       "V13       0\n",
       "V14       0\n",
       "V15       0\n",
       "V16       0\n",
       "V17       0\n",
       "V18       0\n",
       "V19       0\n",
       "V20       0\n",
       "V21       0\n",
       "V22       0\n",
       "V23       0\n",
       "V24       0\n",
       "V25       0\n",
       "V26       0\n",
       "V27       0\n",
       "V28       0\n",
       "V29       0\n",
       "Target    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fffff6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check the target variable distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f5aef7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    56864\n",
       "1       98\n",
       "Name: Target, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d058a05",
   "metadata": {},
   "source": [
    "## We have a highly imbalanced dataset \n",
    "we will use stratify to maintain the distribution of class in the traina and test \n",
    "\n",
    "Accuracy is not a good evaluation metric for imbalanced datasets because it can be misleading. \n",
    "\n",
    "Accuracy is the ratio of the correctly predicted observations to the total observations. In an imbalanced dataset, where one class significantly outnumbers the other, a model can achieve a high accuracy rate by simply predicting the majority class for all observations. \n",
    "\n",
    "For example, consider a dataset where 95% of the instances belong to Class A and only 5% belong to Class B. A model that always predicts Class A will have an accuracy of 95%, even though it fails to correctly predict any instances of Class B.\n",
    "\n",
    "Therefore, for imbalanced datasets, other metrics like Precision, Recall, F1-score or Area Under the Receiver Operating Characteristic Curve (AUC-ROC) are often more informative as they take both false positives and false negatives into account. These metrics provide a more balanced view of the model's performance across all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7720486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# independent variable \n",
    "X=data.iloc[:,:-1]\n",
    "\n",
    "# dependent variable\n",
    "Y = data.iloc[:,-1] # only the last column but all the rows \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2e801ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0 , stratify = Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc6402e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((45569, 29), (11393, 29))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b64b028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0    45491\n",
       " 1       78\n",
       " Name: Target, dtype: int64,\n",
       " 0    11373\n",
       " 1       20\n",
       " Name: Target, dtype: int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# since we have a imbalance dataset lets check the distribution of class\n",
    "Y_train.value_counts(),Y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936149d6",
   "metadata": {},
   "source": [
    "# Random forest \n",
    "It is a bootstrap aggregating model we are sample the data with replacement and running decision tree and then we are taking the mean/median/mode of the result and returing the class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "808995d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=42)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's fit the model on the data \n",
    "rf_classifier = RandomForestClassifier(n_estimators = 100, random_state = 42)\n",
    "rf_classifier.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22aa50b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = rf_classifier.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52945914",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = rf_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "807e8a0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[45491,     0],\n",
       "       [    0,    78]], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(Y_train,y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "48e57bc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11370,     3],\n",
       "       [    5,    15]], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(Y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8b1acac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(Y_train,y_pred_train) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "babbd2e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.92978144474678"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(Y_test,y_pred_test) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a28e3e",
   "metadata": {},
   "source": [
    "## Accuracy is not a good metric to judge the performance of the class\n",
    "\n",
    "why \n",
    "\n",
    "This is why\n",
    "\n",
    "Alright, imagine you have a big jar of colorful candies, and most of them are red candies, but there are just a few green candies mixed in. You want to know if your friend can pick out the green candies correctly.\n",
    "\n",
    "Now, let's say your friend really likes red candies and decides to guess that every candy they pick is red because there are so many of them. Your friend is really good at picking red candies, so they get most of them right. But, they completely miss the green candies because they never even try to pick them.\n",
    "\n",
    "Now, if we just count how many candies your friend got right (the red ones), it looks like they did a great job because there are so many red candies. That's like using accuracy as a metric. But, we also wanted them to find the green candies, and they completely missed those. So, accuracy doesn't tell us the whole story because it doesn't show that they missed something important.\n",
    "\n",
    "In a similar way, when you have imbalanced data where one group is much smaller than the other, just looking at accuracy can make it seem like your model is doing well, even if it's not doing a good job at finding the smaller group (like the green candies). That's why we use other ways, like \"recall,\" to make sure we're not missing important things, even if they are rare."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374c3736",
   "metadata": {},
   "source": [
    "# roc = reciever operator charaterstics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c57231d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(Y_train,y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "85c65a49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8748681086784489"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(Y_test,y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "110ce6dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45569, 29)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "24b5b74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_probability=rf_classifier.predict_proba(X_train)\n",
    "y_test_probability = rf_classifier.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e289d264",
   "metadata": {},
   "source": [
    "The graph is more close to the top left this means our model is able to classify 0 as 0 and 1 as 1 , \n",
    "\n",
    "The proprotion of correctly classified positive class is more then the proprotion of incorrectly classifed positive "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a118106d",
   "metadata": {},
   "source": [
    "# AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766f0b89",
   "metadata": {},
   "source": [
    "AUC is area under the curve it is ploted between True positive rate and False positive rate the True positive rate is recall of positive class but the False positive rate is not recall \n",
    "\n",
    "false positive rate is, you classified the class as positive and how many times were you wrong in your prediction \n",
    "\n",
    "True positive rate is total ture postive predicted / totla no. of positive class i.e True positive/Total positive class. How many were you able to recall the actual class is True positive rate \n",
    "\n",
    "False negative rate is False positive  /FP + TN(total actual negative class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b612bfe7",
   "metadata": {},
   "source": [
    "AUC is a graph between True positive rate vs False positive rate at different values of thresholds \n",
    "\n",
    "\n",
    "**In AUC the if the curve is close to the left top then the porportion of rightly classified class 1 is more then the proportion of worngly classifed class 1**\n",
    "\n",
    "**If the value of AUC is close to 1 then our model correctly classifys 0 as 0 and 1 as 1**\n",
    "\n",
    "**if the value of AUC is 0.5 it means it random classifys the data point as 0 and 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a5c712",
   "metadata": {},
   "source": [
    "**ROC is basically curve between True positive rate(recall of positive class) vs False positive rate**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2302a6f1",
   "metadata": {},
   "source": [
    "# Gradient boosting algrothim "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf3a8e4",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=Nol1hVtLOSg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4effc02",
   "metadata": {},
   "source": [
    "In gradient boosting we pass the data set through a basemodel and get the output then we pass the error and features through decisoin tree that decision tree gives residual as output and we subtract the residual from actual to get the target but in such case there is a probelm of overfitting so we multiply the residual with the learning rate and this process is continued "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517346a2",
   "metadata": {},
   "source": [
    "Gradient Boosting is a machine learning technique that can be used with decision trees (among other base learners) for both classification and regression tasks. The most popular variant of gradient boosting with decision trees is called Gradient Boosted Trees or Gradient Boosting Machines (GBM). Here's how it works:\n",
    "\n",
    "1. **Initialize the Model**: Gradient Boosting starts with an initial prediction, which is often a simple estimate like the mean (for regression) or the log-odds (for binary classification) of the target variable.\n",
    "\n",
    "2. **Calculate Residuals or Gradients**: In each iteration, the model calculates the residuals or gradients. For regression tasks, this is the difference between the actual target values and the current predictions. For classification tasks, it is the negative gradient of the log-likelihood loss function with respect to the predictions.\n",
    "\n",
    "3. **Train a Weak Learner (Decision Tree)**: A weak learner, typically a shallow decision tree (often referred to as a \"base learner\" or \"base estimator\"), is trained to fit the residuals or gradients from the previous step. The goal of this step is to find a tree that can correct the errors or misclassifications made by the current model.\n",
    "\n",
    "4. **Update the Model**: The output of the weak learner is scaled by a learning rate (a small number between 0 and 1) and added to the current model. This update is usually performed in small steps to avoid overfitting. The learning rate controls how much each tree contributes to the final model.\n",
    "\n",
    "5. **Repeat**: Steps 2 to 4 are repeated for a predefined number of iterations or until a convergence criterion is met.\n",
    "\n",
    "6. **Final Prediction**: The final prediction is the cumulative sum of the predictions from all the weak learners. For regression, this is the sum of the initial prediction and the contributions from all the decision trees. For classification, it's the logistic transformation of the sum.\n",
    "\n",
    "The key idea behind gradient boosting is that each new decision tree is trained to correct the errors made by the current ensemble of trees. By iteratively adding trees, the model gradually reduces the residuals and improves its predictive accuracy. The learning rate controls the step size of this process.\n",
    "\n",
    "Gradient Boosting is a powerful ensemble method known for its ability to handle complex relationships in data and provide accurate predictions. It is widely used in machine learning for various tasks due to its robustness and flexibility. Popular libraries like XGBoost, LightGBM, and scikit-learn's GradientBoostingClassifier/GradientBoostingRegressor implement gradient boosting with decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7fad27",
   "metadata": {},
   "source": [
    "**AdaBoost focuses on modifying data point weights to emphasize misclassified examples, while Gradient Boosting focuses on updating the model itself by fitting new models to the residuals or gradients. This fundamental difference in updating strategy leads to variations in their behavior and performance.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91c554d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98ab2d2e",
   "metadata": {},
   "source": [
    "Certainly! Let's walk through examples for both regression and binary classification tasks to illustrate the calculation of residuals or negative gradients in Gradient Boosting.\n",
    "\n",
    "**Example 1: Regression Task**\n",
    "\n",
    "Suppose we have a simple regression problem where we want to predict the prices of houses based on their sizes. Here's a small dataset:\n",
    "\n",
    "```\n",
    "| Size (x) | Price (y) |\n",
    "|----------|-----------|\n",
    "|   1.1    |    100    |\n",
    "|   1.5    |    150    |\n",
    "|   2.0    |    200    |\n",
    "|   2.5    |    250    |\n",
    "```\n",
    "\n",
    "Let's assume we start with an initial prediction of the mean price (e.g., $175). In the first iteration, we calculate the residuals:\n",
    "\n",
    "- For the first data point: \\( \\text{Residual}_1 = 100 - 175 = -75 \\)\n",
    "- For the second data point: \\( \\text{Residual}_2 = 150 - 175 = -25 \\)\n",
    "- For the third data point: \\( \\text{Residual}_3 = 200 - 175 = 25 \\)\n",
    "- For the fourth data point: \\( \\text{Residual}_4 = 250 - 175 = 75 \\)\n",
    "\n",
    "These residuals represent the differences between the true target values and the current predictions.\n",
    "\n",
    "**Example 2: Binary Classification Task**\n",
    "\n",
    "Suppose we have a binary classification problem where we want to classify emails as spam (1) or not spam (0) based on the number of exclamation marks they contain. Here's a small dataset:\n",
    "\n",
    "```\n",
    "| Email (x) | Actual (y) |\n",
    "|-----------|------------|\n",
    "|     2     |     1      |\n",
    "|     3     |     0      |\n",
    "|     1     |     1      |\n",
    "|     4     |     0      |\n",
    "```\n",
    "\n",
    "Let's assume we start with an initial prediction of the log-odds (logit) of the mean probability (e.g., logit(0.5) = 0). In the first iteration, we calculate the negative gradients of the log-likelihood loss:\n",
    "\n",
    "- For the first data point (spam): \\( \\text{Negative Gradient}_1 = p_1 - 1 = \\sigma(0) - 1 = 0.5 - 1 = -0.5 \\)\n",
    "- For the second data point (not spam): \\( \\text{Negative Gradient}_2 = p_2 - 0 = \\sigma(0) - 0 = 0.5 \\)\n",
    "- For the third data point (spam): \\( \\text{Negative Gradient}_3 = p_3 - 1 = \\sigma(0) - 1 = 0.5 - 1 = -0.5 \\)\n",
    "- For the fourth data point (not spam): \\( \\text{Negative Gradient}_4 = p_4 - 0 = \\sigma(0) - 0 = 0.5 \\)\n",
    "\n",
    "Here, \\( p_i \\) represents the predicted probability that the \\( i \\)th email is spam (1), and \\(\\sigma(z)\\) is the sigmoid function used to transform the log-odds into probabilities.\n",
    "\n",
    "These negative gradients represent how much the current model's predictions deviate from the true labels and guide the training of the next weak learner in Gradient Boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60988a3",
   "metadata": {},
   "source": [
    "# Overfitting issue \n",
    "Gradient Boosting is a powerful ensemble learning technique, but like any machine learning method, it's susceptible to overfitting if not used appropriately. Adding too many trees to a Gradient Boosting model can indeed lead to overfitting, especially if the model becomes excessively complex and starts to fit the noise in the training data. Here are some considerations:\n",
    "\n",
    "1. **Complexity of Weak Learners**: Gradient Boosting uses decision trees as its weak learners. If the trees are too deep or have too many nodes, they can become highly complex and overfit the training data. To mitigate this, it's common to use shallow trees (e.g., limiting the depth or the number of nodes) or to control tree complexity through hyperparameters like `max_depth` and `min_samples_split`.\n",
    "\n",
    "2. **Learning Rate**: The learning rate (often denoted as `learning_rate`) controls the contribution of each tree to the final ensemble. Smaller learning rates require more trees to achieve the same performance but can make the model more robust against overfitting. You can experiment with different learning rates to find an appropriate balance.\n",
    "\n",
    "3. **Early Stopping**: Implementing early stopping is a common technique to prevent overfitting. You monitor the model's performance on a validation dataset during training, and when the performance starts to degrade, you stop adding trees. This prevents the model from continuing to fit the noise in the training data.\n",
    "\n",
    "4. **Cross-Validation**: Use cross-validation techniques, such as k-fold cross-validation, to evaluate the model's performance on different subsets of the data. This helps you assess whether the model generalizes well to unseen data and can give you insights into whether the number of trees is reasonable.\n",
    "\n",
    "5. **Regularization**: Some implementations of Gradient Boosting (e.g., XGBoost and LightGBM) offer regularization options like `colsample_bytree` and `colsample_bylevel` to control overfitting by subsampling features or nodes in each tree.\n",
    "\n",
    "6. **Data Size**: Overfitting is more likely to occur when you have a small dataset. If your dataset is limited in size, you may want to be more conservative with the number of trees.\n",
    "\n",
    "7. **Pruning**: Some implementations allow for post-pruning of trees. Pruning can help reduce the complexity of individual trees and prevent overfitting.\n",
    "\n",
    "In summary, while Gradient Boosting can build complex models, it provides several mechanisms to control overfitting. Regularization, controlling tree depth, adjusting the learning rate, and using techniques like early stopping and cross-validation are essential practices to strike the right balance between model complexity and generalization performance. It's important to experiment and tune hyperparameters to find the best settings for your specific problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b6d6e9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the gradient boosting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0195649e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(max_depth=2, n_estimators=20, random_state=0)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "classifier = GradientBoostingClassifier(n_estimators = 20, max_depth = 2, learning_rate = 0.1, random_state = 0)\n",
    "classifier.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bb529755",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b3c8bff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8246922535830475"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(Y_test,y_pred_test)\n",
    "# this models are prone to over fit "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1c00e7",
   "metadata": {},
   "source": [
    "ROC score is more in case of bagging algrothim - random forest "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e572c7d8",
   "metadata": {},
   "source": [
    "# XGboost "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0316f2e5",
   "metadata": {},
   "source": [
    "regressor = https://www.youtube.com/watch?v=w-_vmVfpssg&t=115s\n",
    "classifier = https://www.youtube.com/watch?v=gPciUPwWJQQ&t=457s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bac0f6",
   "metadata": {},
   "source": [
    "regressor we use the variance reduction  in classifier we use gini  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "23bb4275",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a7ab5e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-2.0.0-py3-none-win_amd64.whl (99.7 MB)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost) (1.21.5)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost) (1.7.3)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-2.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "fba82d49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=2, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=20, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=0, ...)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgboost=XGBClassifier(n_estimators = 20, max_depth = 2, learning_rate = 0.1, random_state = 0)\n",
    "xgboost.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5e317eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred=xgboost.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1fb2d3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the auc is 0.8998241449045985\n"
     ]
    }
   ],
   "source": [
    "print(f' the auc is {roc_auc_score(Y_test, test_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953b7b58",
   "metadata": {},
   "source": [
    "Working is similar to grdient boosting but it is specically desigined to enhance the performance for classification and regression problem , The data set is passed through a base line mode then the residuals and features are passed into the decision tree and learning rate is used to avoid the overfit issue in classification the output of the base line model will be 0.5 we take the residuals and pass the residuals and features now the split happens which ever feature gives the best similiarity score( information gain) that feature is decided , which every split value gives the best similarity score( information gain) that split value is choose and the this step happens recursively "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b889737c",
   "metadata": {},
   "source": [
    "# ADA boost "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57fffbb",
   "metadata": {},
   "source": [
    "in adataptive boosting each datapoint in the dataset is assigned weights at the start which is 1/no. of data points NOw this data with weights is passed through a stump(decision tree with one split only) and data points are classified\n",
    "\n",
    "now weights of data points are calculated and more weights are assigned to data points which are incorrectly classified and less weights are assigned to data points which are correctly classifed by using mathematical function after the weight of the data points are undated after first stump the a weight random sampling is done ( in weighted random sampling the probability of get a data point with high weight is more then the probability of low weights this way we have reoccurence of data points which are wrongly classifed are more) now agian those which are miss classifed , the weights are updated , random sampling is done again and the weight sample data is passed to a new stump "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0ee815",
   "metadata": {},
   "source": [
    "In the start the stump are created for every features but that feature is choosed which gives maximum redcution in entropy/information gain "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219d13a6",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=NLRO1-jp5F8&t=142s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3b4abb",
   "metadata": {},
   "source": [
    "## Let's fit ada boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b3d27345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
       "                   learning_rate=1, n_estimators=30, random_state=1)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "base_classifier = DecisionTreeClassifier(max_depth = 1)\n",
    "\n",
    "ada_classifier = AdaBoostClassifier(base_estimator = base_classifier, n_estimators = 30, learning_rate = 1 , random_state = 1)\n",
    "# learning rate is the same which is used in gradient boosting and xgboosting there, it regulate the residuals here weights  \n",
    "ada_classifier.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "83bb4637",
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_test_pred = ada_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "f370eb48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.824736217356898"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(Y_test,ada_test_pred )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4061cdbe",
   "metadata": {},
   "source": [
    "# Let's run a CV "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00d33c0",
   "metadata": {},
   "source": [
    "When you run a `GridSearchCV` on a Random Forest classifier (or any other estimator), it will return an instance of the `GridSearchCV` class, which contains information about the results of the hyperparameter search and the best estimator found. Here are some of the key attributes and methods of the `GridSearchCV` object:\n",
    "\n",
    "1. **`best_params_`**: This attribute stores the hyperparameters that resulted in the best performance on the chosen scoring metric.\n",
    "\n",
    "2. **`best_estimator_`**: This attribute holds the best estimator (model) trained with the best hyperparameters found during the search.\n",
    "\n",
    "3. **`best_score_`**: This attribute provides the score achieved by the best estimator on the validation data.\n",
    "\n",
    "4. **`cv_results_`**: This attribute is a dictionary containing detailed information about the cross-validation results for all combinations of hyperparameters. It includes mean and standard deviation scores for each combination.\n",
    "\n",
    "5. **`grid_scores_`** (deprecated in newer versions of scikit-learn): This attribute used to store the cross-validation scores for all combinations of hyperparameters. It's deprecated in favor of `cv_results_` in newer scikit-learn versions.\n",
    "\n",
    "6. **`predict`**: You can use the `predict` method of the `GridSearchCV` object to make predictions using the best estimator found.\n",
    "\n",
    "7. **`predict_proba`**: If the estimator supports it (e.g., for classification tasks), you can use the `predict_proba` method to get class probabilities using the best estimator.\n",
    "\n",
    "Here's an example of how to use these attributes:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 20, 30],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit the grid search to your training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Access the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Access the best estimator\n",
    "best_estimator = grid_search.best_estimator_\n",
    "\n",
    "# Access the best score achieved during cross-validation\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "# Access the detailed cross-validation results\n",
    "cv_results = grid_search.cv_results_\n",
    "\n",
    "# Make predictions using the best estimator\n",
    "y_pred = best_estimator.predict(X_test)\n",
    "```\n",
    "\n",
    "In summary, `GridSearchCV` returns the best hyperparameters, the best estimator, and other useful information related to the hyperparameter tuning process, which can be used for further analysis and model deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "03646037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest had a accuracy of .8748"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7e610ae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestClassifier(),\n",
       "             param_grid={'max_depth': [3, 5, 7], 'n_estimators': [50, 80, 100]},\n",
       "             scoring='roc_auc')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "classifier = RandomForestClassifier()\n",
    "grid_values = {'n_estimators':[50, 80,  100], 'max_depth':[3, 5, 7]} # n_estimators is no. of trees you want ot make \n",
    "classifier = GridSearchCV(classifier, param_grid = grid_values, scoring = 'roc_auc',cv = 5)\n",
    "classifier.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "56767f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9634391380460136"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "20c1ec26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 7, 'n_estimators': 100}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "60622d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = classifier.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a0940d2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8499560362261496"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(Y_test,classifier.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a3daf247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8499560362261496"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(Y_test,best_model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40da1a91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
