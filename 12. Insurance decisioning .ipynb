{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dab4a9d",
   "metadata": {},
   "source": [
    "# Insurance decisioning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f42b0c",
   "metadata": {},
   "source": [
    "The concepts "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22d1e9d",
   "metadata": {},
   "source": [
    "# KS statistics "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ee0c44",
   "metadata": {},
   "source": [
    "In the context of machine learning (ML), the KS statistic can also be relevant, although it's not as commonly used as in statistical hypothesis testing. In ML, the KS statistic is often used in the following contexts:\n",
    "\n",
    "1. **Feature Selection**: The KS statistic can be used to measure the difference in the distribution of a feature (variable) between different classes or groups in a classification problem. If the KS statistic is high, it suggests that the feature is a good discriminator between classes and may be a useful feature for building a predictive model.\n",
    "\n",
    "2. **Model Evaluation**: In binary classification problems, the KS statistic can be used to evaluate the performance of a predictive model, such as a logistic regression model or a random forest classifier. It measures the model's ability to distinguish between the positive and negative classes by comparing the predicted probabilities for the two classes. A higher KS statistic indicates better model discrimination.\n",
    "\n",
    "3. **Credit Scoring**: In credit scoring and risk modeling, the KS statistic is commonly used to assess the effectiveness of a credit scoring model in separating good credit applicants from bad ones. It helps determine the optimal cutoff point for classifying applicants into different risk categories.\n",
    "\n",
    "4. **Anomaly Detection**: The KS statistic can be used in anomaly detection to assess how well a model or algorithm can distinguish between normal and anomalous data points. A higher KS statistic indicates better anomaly detection performance.\n",
    "\n",
    "5. **Feature Engineering**: Data scientists and machine learning practitioners may use the KS statistic to create new features by comparing the distributions of different variables and identifying relationships that could be useful for modeling.\n",
    "\n",
    "While the KS statistic has applications in ML, it's important to note that its use is primarily focused on comparing distributions and evaluating the discrimination power of features or models. It's just one of many statistical and mathematical tools available to data scientists and machine learning practitioners for various tasks, including feature selection, model evaluation, and data preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806907ce",
   "metadata": {},
   "source": [
    "# We will be covering the model evaluation in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb89f4a5",
   "metadata": {},
   "source": [
    "let's dive deeper into how the KS statistic is used in the context of model evaluation for binary classification problems:\n",
    "\n",
    "1. **Understanding Binary Classification**: In binary classification, the goal is to classify data points into one of two classes: the positive class (e.g., presence of a disease) and the negative class (e.g., absence of a disease). A predictive model assigns a probability score or a class label to each data point, indicating the likelihood of it belonging to the positive class.\n",
    "\n",
    "2. **Predicted Probabilities**: For each data point, a binary classification model, such as logistic regression or a random forest classifier, generates predicted probabilities. These probabilities represent the model's confidence that a data point belongs to the positive class. Typically, a threshold is set (e.g., 0.5), and data points with predicted probabilities above this threshold are classified as positive, while those below are classified as negative.\n",
    "\n",
    "3. **Cumulative Distribution Functions (CDFs)**: To compute the KS statistic, you first sort the data points by their predicted probabilities in descending order. Then, you calculate the cumulative distribution functions (CDFs) separately for the positive and negative classes. These CDFs represent the cumulative proportion of true positive (sensitivity) and true negative (specificity) predictions as you move along the sorted list of data points.\n",
    "\n",
    "4. **KS Statistic Calculation**: The KS statistic is calculated as the maximum vertical distance between the two CDFs. In other words, it's the maximum difference between the cumulative proportions of true positives and true negatives across all possible thresholds. A higher KS statistic indicates that the model does a better job of distinguishing between the positive and negative classes.\n",
    "\n",
    "5. **Choosing the Optimal Threshold**: In practice, the KS statistic is often used to determine the optimal threshold for classifying data points. You can select the threshold that corresponds to the maximum KS statistic. This threshold maximizes the discrimination between the two classes and, therefore, can lead to better model performance.\n",
    "\n",
    "6. **Interpretation**: If you have a high KS statistic, it means that the model is effective at separating positive and negative instances. Conversely, a low KS statistic suggests that the model's predicted probabilities do not discriminate well between the two classes. This information is valuable for assessing the model's overall performance and making decisions about threshold selection.\n",
    "\n",
    "7. **Model Comparison**: You can also use the KS statistic to compare the discrimination power of different models. For example, if you have two logistic regression models, you can compute the KS statistic for each and choose the one with the higher value as it is better at distinguishing between classes.\n",
    "\n",
    "In summary, the KS statistic is a useful tool for evaluating and fine-tuning binary classification models. It helps in setting an appropriate classification threshold and provides insight into how well the model separates positive and negative instances, making it an important metric in model assessment and selection.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f359c5aa",
   "metadata": {},
   "source": [
    "**The ensemble models can handle correaltion very well so we don't have to worry about it**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bf3ae6",
   "metadata": {},
   "source": [
    "When handling the missing value do it on the train data set like filling the values with the mean of train variable in the train now use that value to fill the test missing values why so because we donn't want to do data leakage that we dont want the fit on test test should be prestine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49356f60",
   "metadata": {},
   "source": [
    "# Bayseain optimization the math and the working "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66743ac6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
